# Clustering Algorithms

Clustering is a fundamental technique that involves<mark style="background: #FFF3A3A6;"> grouping similar data points together based on their intrinsic characteristics.</mark>

> [!example]
> An e-commerce company may employ clustering to identify segments like "Frequent Shoppers," "Occasional Shoppers," "High-Spending Customers," and "Budget Shoppers."


## Advantages of Clustering

1. **Data Exploration:** Clustering is a valuable tool for <mark style="background: #BBFABBA6;">exploring and understanding data patterns</mark>, helping analysts uncover hidden structures within the dataset.
    
2. **Unsupervised Learning:** Clustering<mark style="background: #BBFABBA6;"> does not require labeled data,</mark> making it suitable for unsupervised learning tasks where class labels are unknown or unavailable.
    
3. **Pattern Discovery:** Clustering can <mark style="background: #BBFABBA6;">reveal natural groupings and patterns in the data</mark>, providing insights into similarities and differences between data points.
    
4. **Data Reduction:** Clustering can be <mark style="background: #BBFABBA6;">used for data dimensionality reduction by grouping similar data points together,</mark> reducing the need for excessive feature engineering.
    
5. **Segmentation:** Businesses can apply clustering to <mark style="background: #BBFABBA6;">segment their customers,</mark> which aids in targeted marketing, product recommendations, and personalized services.
    
6. **Anomaly Detection:** Clustering can help <mark style="background: #BBFABBA6;">identify outliers or anomalies </mark>that do not belong to any cluster, making it useful for detecting rare events or errors.
    
7. **Simplicity:** Many clustering algorithms are <mark style="background: #BBFABBA6;">relatively simple and computationally efficient</mark>, making them accessible for various applications.
    

## Disadvantages of Clustering

1. **Ambiguity in Cluster Number:** <mark style="background: #BBFABBA6;">Determining the optimal number of clusters (K) can be challenging</mark> and often requires manual selection or the use of heuristics, leading to potential subjectivity.
    
2. **Sensitivity to Initialization:** Some clustering algorithms, such as K-Means, are <mark style="background: #BBFABBA6;">sensitive to initial centroid placement,</mark> which can result in different outcomes with different initializations.
    
3. **Scalability:** Clustering large datasets can be <mark style="background: #BBFABBA6;">computationally expensive</mark> and time-consuming, requiring specialized algorithms and hardware.
    
4. **Curse of Dimensionality:** Clustering can become <mark style="background: #BBFABBA6;">less effective in high-dimensional spaces</mark> due to the increased distance between data points.

## K-Means Clustering

K-Means clustering is a partitioning algorithm  to <mark style="background: #FFF3A3A6;">divide a dataset into distinct, non-overlapping groups or clusters.</mark>


![[Pasted image 20231102200045.png]]

### Process

K-Means clustering works through an iterative process that aims to partition a dataset into K distinct clusters. The algorithm minimizes the sum of squared distances between data points and their respective cluster centers. Here's a step-by-step explanation of how K-Means works:

1. **Initialization:**
    
    - Choose the number of clusters, K, that you want to divide your data into.
    - Initialize K cluster centers (centroids). This can be done randomly or using a specific initialization method (e.g., K-Means++ initialization).
    - The initial centroids represent the starting points for the clusters.
2. **Assignment:**
    
    - For each data point in the dataset, calculate the distance (typically Euclidean distance) to each of the K cluster centers.
    - Assign the data point to the cluster whose centroid is closest (i.e., the one with the minimum distance).
    - Each data point is associated with one and only one cluster based on this distance metric.
3. **Update:**
    
    - Recalculate the cluster centers (centroids) by taking the mean of all data points assigned to each cluster.
    - The updated centroids represent the new center of each cluster.
4. **Convergence Check:**
    
    - Repeat the Assignment and Update steps iteratively until one of the stopping criteria is met:
        - No data points change their cluster assignments.
        - The centroids no longer move significantly.
        - A predefined number of iterations is reached.
5. **Result:**
    
    - Once the algorithm converges, the final cluster centers represent the centroids of the K clusters.
    - Each data point belongs to one of these clusters based on its assignment during the final iteration.


### Applications

1. **<mark style="background: #ADCCFFA6;">Customer Segmentation</mark>:** Businesses use K-Means to <mark style="background: #BBFABBA6;">group customers into segments</mark> based on their purchasing behavior, demographics, or preferences. 
    
2. **<mark style="background: #ADCCFFA6;">Image Compression</mark>:** K-Means can be used to <mark style="background: #BBFABBA6;">compress images by reducing the number of colors</mark> (cluster centroids) while maintaining image quality. 
    
3. **<mark style="background: #ADCCFFA6;">Anomaly Detection</mark>:** K-Means can be employed for <mark style="background: #BBFABBA6;">identifying anomalies or outliers in datasets.</mark> 
    
4. **<mark style="background: #ADCCFFA6;">Document Clustering</mark>:** In natural language processing, K-Means is used to <mark style="background: #BBFABBA6;">cluster documents, such as news articles or customer reviews, based on their content.</mark> 
    
5. **<mark style="background: #ADCCFFA6;">Genomics</mark>:** K-Means can be used to <mark style="background: #BBFABBA6;">cluster gene expression data</mark>, helping in the identification of gene patterns and related biological processes.


## Hierarchical Clustering

Hierarchical clustering is a type of clustering algorithm used in machine learning and data analysis to <mark style="background: #FFF3A3A6;">build a hierarchy of clusters within a dataset.</mark>


**Distance Metrics:**

- **<mark style="background: #ADCCFFA6;">Euclidean Distance</mark>:** Euclidean distance <mark style="background: #BBFABBA6;">measures the straight-line distance between two points in a multi-dimensional space.</mark> 
    
- **<mark style="background: #ADCCFFA6;">City Block Distance (Manhattan Distance)</mark>:** City block distance calculates the distance as <mark style="background: #BBFABBA6;">the sum of the absolute differences along each dimension.</mark> 
    
- **Other Distance Metrics:** Depending on the domain and nature of the data, other distance metrics such as <mark style="background: #ABF7F7A6;">Mahalanobis distance</mark>, <mark style="background: #ABF7F7A6;">cosine similarity</mark>, <mark style="background: #ABF7F7A6;">Jaccard distance</mark>, and <mark style="background: #ABF7F7A6;">correlation distance</mark> may be more appropriate. 
    

**Linkage Criteria:**

- **Single-Linkage (Minimum Linkage):** In single-linkage clustering, the <mark style="background: #BBFABBA6;">distance between two clusters is defined as the minimum distance between any two data points, one from each cluster.</mark> This can lead to "chaining" effects, where clusters are elongated and may merge even if just a single data point is close.
    
- **Complete-Linkage (Maximum Linkage):** Complete-linkage clustering calculates <mark style="background: #BBFABBA6;">the distance between two clusters as the maximum distance between any two data points</mark>, one from each cluster. It can lead to the creation of compact, spherical clusters.
    
- **Average-Linkage (Mean Linkage):** Average-linkage computes <mark style="background: #BBFABBA6;">the distance between two clusters as the average of all pairwise distances between data points in the two clusters.</mark> 
    
- **Other Linkage Criteria:** <mark style="background: #ABF7F7A6;">Ward's linkage</mark>, <mark style="background: #ABF7F7A6;">centroid linkage</mark>, and <mark style="background: #ABF7F7A6;">median linkage</mark> are examples of other linkage criteria that offer different trade-offs between sensitivity to outliers and cluster shape.

**Agglomerative (Bottom-Up) Hierarchical Clustering:**

1. **Initialization:** Treat each data point as a single cluster, resulting in as many clusters as there are data points.
2. **Similarity Measure:** Calculate the pairwise similarity (distance) between all clusters (e.g., single linkage, complete linkage, or average linkage).
3. **Merge:** Merge the two most similar clusters into a single cluster, reducing the total number of clusters by one.
4. **Repeat:** Recalculate the similarities between the remaining clusters and iteratively merge the closest clusters until only one cluster remains, forming the dendrogram.

**Divisive (Top-Down) Hierarchical Clustering:**

1. **Initialization:** Start with all data points in a single cluster, representing the entire dataset.
2. **Similarity Measure:** Calculate the similarity (distance) between data points within the cluster.
3. **Split:** Split the cluster into two smaller clusters by dividing it along the least similar data points.
4. **Repeat:** Recalculate the similarities within the newly formed clusters, iteratively splitting clusters until each data point is in its own cluster.


