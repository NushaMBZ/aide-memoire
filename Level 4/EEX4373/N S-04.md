# Model Fitting

It is the process of <mark style="background: #FFF3A3A6;">training a model on a dataset.</mark>

> [!definition]
> A model is a computational algorithm or a network of algorithms that is trained on data to learn patterns and relationships.

## Model Over-fitting

MO occurs when a <mark style="background: #FFF3A3A6;">model learns to perform exceptionally well on the training data but fails to generalize effectively to new, unseen data.</mark>

Key characteristics and causes of overfitting include:

1. **<mark style="background: #ADCCFFA6;">Complexity of the Model</mark>:** Overfitting often occurs when a <mark style="background: #BBFABBA6;">model is too complex relative to the amount of training data available.</mark> 
    
2. **<mark style="background: #ADCCFFA6;">Insufficient Data</mark>:** Having <mark style="background: #BBFABBA6;">a small or insufficient amount of training data can make it easier for a model to overfit.</mark> With limited data, the model may find it tempting to rely on noise or outliers in the data, as there are not enough examples to discern true patterns.
    
3. **<mark style="background: #ADCCFFA6;">Noise in the Data</mark>:** If the <mark style="background: #BBFABBA6;">training data contains noise, errors, or outliers, the model may mistakenly try to fit those irregularities,</mark> resulting in poor generalization.
    

To address overfitting and improve model generalization, several techniques can be employed:

1. **<mark style="background: #ADCCFFA6;">Simplifying the Model</mark>:** <mark style="background: #BBFABBA6;">Reduce the complexity of the model.</mark> For example, use a shallower neural network, lower-order polynomial regression, or fewer decision tree nodes.
    
2. **<mark style="background: #ADCCFFA6;">Collect More Data</mark>:** <mark style="background: #BBFABBA6;">Obtaining additional data</mark> can help the model learn the underlying patterns more effectively, reducing the risk of overfitting.
    
3. **<mark style="background: #ADCCFFA6;">Cross-Validation</mark>:** <mark style="background: #BBFABBA6;">Implement techniques like k-fold cross-validation</mark> to assess the model's performance on multiple subsets of the data. This can give a more robust evaluation and help detect overfitting.
    
4. **<mark style="background: #ADCCFFA6;">Regularization</mark>:** Techniques like L1 or L2 regularization can <mark style="background: #BBFABBA6;">penalize large model coefficients and help prevent overfitting.</mark>
    
5. **<mark style="background: #ADCCFFA6;">Feature Selection</mark>:** Carefully <mark style="background: #BBFABBA6;">choose relevant features</mark> and discard irrelevant or noisy ones.
    
6. **<mark style="background: #ADCCFFA6;">Early Stopping</mark>:** <mark style="background: #BBFABBA6;">Monitor the model's performance on a validation dataset during training, and stop training when the performance on the validation data starts to degrade</mark>, indicating overfitting.

## Model Underfitting

MU occurs when a model is <mark style="background: #FFF3A3A6;">too simple to capture the underlying patterns in the training data, resulting in poor performance both on the training data and on new, unseen data.</mark>

Key characteristics and causes of underfitting include:

1. **<mark style="background: #ADCCFFA6;">Simplicity of the Model</mark>:** An underfit model is often <mark style="background: #BBFABBA6;">too simple or lacks the capacity to capture the complexity of the underlying relationships in the data.</mark> 
    
2. **<mark style="background: #ADCCFFA6;">Inadequate Features</mark>:** In some cases, the <mark style="background: #BBFABBA6;">model may not have access to all the relevant features or variables necessary to understand the problem.</mark> 
    
3. **<mark style="background: #ADCCFFA6;">Insufficient Model Training</mark>:** If the <mark style="background: #BBFABBA6;">model has not been trained for a sufficient number of iterations or epochs</mark>, it may not have had the opportunity to learn the data's underlying patterns.
    
4. **<mark style="background: #ADCCFFA6;">Inadequate Data</mark>:** The amount of <mark style="background: #BBFABBA6;">training data can also contribute to underfitting. </mark>If the dataset is too small or unrepresentative of the problem, the model may fail to generalize effectively.
    

To address underfitting and improve model performance, you can consider the following strategies:

1. **<mark style="background: #ADCCFFA6;">Increase Model Complexity</mark>:** If the model is too simple, consider <mark style="background: #BBFABBA6;">using a more complex model that can better capture the data's underlying patterns.</mark> 
    
2. **<mark style="background: #ADCCFFA6;">Feature Engineering</mark>:** Carefully <mark style="background: #BBFABBA6;">select and engineer features that are relevant to the problem</mark>, which can help the model better represent the underlying relationships.
    
3. **<mark style="background: #ADCCFFA6;">More Data</mark>:** <mark style="background: #BBFABBA6;">Collecting more data</mark>, if feasible, can provide the model with a richer dataset to learn from and improve generalization.
    
4. **<mark style="background: #ADCCFFA6;">Model Training</mark>:** Ensure that <mark style="background: #BBFABBA6;">the model is trained for an appropriate number of iterations or epochs</mark>, allowing it to converge to a good solution.
    
5. **<mark style="background: #ADCCFFA6;">Hyperparameter Tuning</mark>:** <mark style="background: #BBFABBA6;">Adjust hyperparameters, such as learning rate, batch size, and regularization strength</mark>, to fine-tune the model's performance.
    
6. **<mark style="background: #ADCCFFA6;">Use a Different Model</mark>:** If the current model is fundamentally incapable of capturing the problem's complexity, <mark style="background: #BBFABBA6;">consider trying a different machine learning algorithm</mark> or modeling approach.

## Bias

"bias" in models refers to <mark style="background: #FFF3A3A6;">systematic errors or inaccuracies in the predictions made by the model.</mark>

Bias can occur for various reasons, including:

1. **<mark style="background: #ADCCFFA6;">Modeling Assumptions</mark>:** If the <mark style="background: #ABF7F7A6;">model's assumptions about the relationship between variables are incorrect or overly simplified</mark>, it can lead to bias.
    
2. **<mark style="background: #ADCCFFA6;">Underfitting</mark>:** When a model <mark style="background: #ABF7F7A6;">is too simple or lacks the complexity to capture the true underlying patterns in the data</mark>, it can systematically underpredict or overpredict the target variable. This is often referred to as underfitting bias.
    
3. **<mark style="background: #ADCCFFA6;">Data Quality</mark>:** Bias can <mark style="background: #ABF7F7A6;">result from errors and inaccuracies in the training data, </mark>such as data entry errors, missing values, or data collection biases. If the training data is biased, the model's predictions may also be biased.

## Variance

"variance" refers to the amount by which the <mark style="background: #FFF3A3A6;">predictions or model outcomes vary for different datasets or data samples.</mark>

The key points regarding variance in the context of machine learning are:

- **High Variance:**
    
    - Indicates that the model is too complex and can fit the training data too closely.
    - Prone to overfitting, which leads to poor generalization.
    - Predictions vary significantly across different training datasets.
- **Low Variance:**
    
    - Suggests that the model is overly simple and inflexible.
    - Tends to underfit the data, failing to capture the true patterns.
    - Predictions are more consistent but may be consistently inaccurate.

### Bias vs Variance

|Aspect|Bias|Variance|
|---|---|---|
|Key Issue|Systematic prediction errors|Prediction variability|
|Underlying Problem|Overly simplistic model|Overly complex model|
|Impact on Training Data|Underestimates patterns|Fits noise in the data|
|Impact on Testing Data|May underfit or be inaccurate|May overfit or be inaccurate|
|Sensitivity to Data|Low|High|
|Generalization|May generalize poorly|May generalize poorly|
|Solution|Increase model complexity, gather more data, address data quality|Decrease model complexity, use regularization, employ cross-validation|
|Trade-off|Inversely related to variance (bias-variance trade-off)|Inversely related to bias (bias-variance trade-off)|
|Example|Linear regression model on a non-linear problem|High-degree polynomial regression on a data with little noise|

![[Pasted image 20231102172429.png]]

|Aspect|Low Bias & High Variance|<mark style="background: #FF5582A6;">Low Bias & Low Variance</mark>|High Bias & Low Variance|High Bias & High Variance|
|---|---|---|---|---|
|Prediction Stability|Less stable, high variation in predictions|Stable, low variation in predictions|Stable, low variation in predictions|Unstable, high variation in predictions|
|Model Flexibility|Highly flexible, adapts to training data|Moderately flexible, strikes a balance|Rigid, does not adapt well to data|Extremely rigid, doesn't adapt to training data|
|Training Data Fit|Fits training data closely, prone to overfitting|Fits training data well without overfitting|Underfits training data, lacks adaptation|Underfits training data, prone to overfitting|
|Generalization|May perform poorly on unseen data, overfits|Likely to perform well on unseen data|May perform poorly on unseen data, underfits|Performs poorly on both training and unseen data|
|Common Fixes|Regularization, more training data|Suitable model complexity, cross-validation|Decrease model complexity, more data|Re-evaluate model choice, feature engineering|
|Trade-off|Inversely related to variance (bias-variance trade-off)|Independent (seek a balance)|Inversely related to bias (bias-variance trade-off)|Independent (often undesirable)|
|Example|Complex deep neural network|Simple linear regression|Overly simplified linear model|Complex, overfit decision tree|

![[Pasted image 20231102175112.png]]

# Regularization

Regularization is a technique in machine learning used to<mark style="background: #FFF3A3A6;"> prevent overfitting in models by adding a penalty term to the model's loss function.</mark>

![[Pasted image 20231102175509.png]]
Here are the primary benefits of regularization:

1. **<mark style="background: #ADCCFFA6;">Preventing Overfitting</mark>:** Regularization helps <mark style="background: #BBFABBA6;">control the model's complexity, reducing the risk of overfitting.</mark> By adding a penalty for large coefficients, it discourages the model from fitting noise in the data.
    
2. **<mark style="background: #ADCCFFA6;">Feature Selection</mark>:** L1 regularization (Lasso) can <mark style="background: #BBFABBA6;">perform automatic feature selection by setting some coefficients to zero.</mark> This is valuable when dealing with high-dimensional data with many irrelevant features.
    
3. **<mark style="background: #ADCCFFA6;">Improved Generalization</mark>:** Regularized models often <mark style="background: #BBFABBA6;">generalize better to new, unseen data </mark>because they are less likely to memorize the training data and more likely to capture the underlying patterns.
    
4. **<mark style="background: #ADCCFFA6;">Reduced Model Complexity</mark>:** Regularization can <mark style="background: #BBFABBA6;">lead to simpler and more interpretable models</mark>, which is often desirable in practice.

Here are some characteristics of regularization:

1. **<mark style="background: #ADCCFFA6;">Penalty Term</mark>:** Regularization techniques <mark style="background: #BBFABBA6;">add a penalty term to the model's loss function, which is designed to constrain the model's parameters </mark>(e.g., coefficients in linear models, weights in neural networks).
    
2. **<mark style="background: #ADCCFFA6;">Strength of Regularization</mark>:** The <mark style="background: #BBFABBA6;">strength of regularization is controlled by a hyperparameter </mark>(often denoted as lambda or alpha) that determines how much the penalty term affects the loss function. Higher values of the hyperparameter result in stronger regularization, which constrains the parameters more.
3. **<mark style="background: #ADCCFFA6;">Trade-off</mark>:** Regularization <mark style="background: #BBFABBA6;">introduces a trade-off between fitting the training data closely and preventing overfitting.</mark> Stronger regularization constrains the model's complexity, but it may result in underfitting if taken to the extreme.
4. **<mark style="background: #ADCCFFA6;">Hyperparameter Tuning</mark>:** The strength of regularization, as determined by the regularization hyperparameter, is typically <mark style="background: #BBFABBA6;">fine-tuned using techniques such as cross-validation</mark> to find the best balance between bias and variance.

## Regularization Techniques

Regularization techniques are methods used to prevent overfitting in machine learning models by adding a penalty to the loss function that constrains the model's parameters. Here are some common regularization techniques:

### **L1 Regularization (Lasso)**
 
L1 regularization <mark style="background: #FFF3A3A6;">adds a penalty term to the loss function that is proportional to the absolute values of the model's coefficients.</mark> It encourages some coefficients to become exactly zero, effectively performing feature selection by eliminating irrelevant features.
$$
L_{reg} = L + \lambda \sum_{i=1}^n ||w_i||

$$
where :
- $L$ is the original loss function
- $λ$ is the regularization strength
- $n$ is the number of weights in the model
- $w_i$​ is the i-th weight
### **L2 Regularization (Ridge)** 

L2 regularization <mark style="background: #FFF3A3A6;">adds a penalty term to the loss function that is proportional to the square of the model's coefficients.</mark> It discourages coefficients from becoming too large and helps prevent any single feature from dominating the model. L2 regularization encourages a more balanced usage of all features.

$$
L_{reg} = L + \lambda \sum_{i=1}^{n} ||w_i||^2
$$
where :
- $L$ is the original loss function
- $λ$ is the regularization strength
- $n$ is the number of weights in the model
- $w_i$​ is the i-th weight