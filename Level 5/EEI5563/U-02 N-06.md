# Computing Benchmarks

In computing, a **benchmark** is <mark style="background: #BBFABBA6;">the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it.</mark>

## Purpose of Benchmarks

- **Quantitative System Evaluation:** Benchmarks provide objective, measurable metrics for performance comparison rather than relying on marketing claims or raw specifications.
- **Hardware-Software Co-Design:** Help architects understand the impact of hardware choices on real-world application workloads, facilitating informed optimizations on both sides of the stack.
- **Bottleneck Identification:** By isolating components or subsystems (CPU, memory, I/O), benchmarks pinpoint performance-limiting factors.
- **Procurement Decisions:** Aid in informed system purchasing by offering apples-to-apples comparisons, especially for large-scale deployments (e.g., data centers).

## Types of Benchmarks

1. Application Benchmarks

- **Description:** Use real-world software to measure performance under representative workloads.
- **Types:**
    - **Word Processing:** Productivity suites (e.g., Microsoft Office, LibreOffice)
    - **CAD Software:** Design & modeling tools (e.g., AutoCAD, Solidworks)
    - **MIS Software:** Management Information Systems for business operations
- **Advantages:** Provide the most direct insight into how systems will perform under realistic usage patterns.
- **Challenges:** Can be setup-intensive and influenced by software/configuration differences.

2. Component/Microbenchmarks

- **Description:** Isolate and measure specific hardware components or low-level operations.
- **Uses:**
    - **Hardware Parameter Detection:** Cache size, registers, memory latency, etc.
    - **Pinpointing Bottlenecks:** Identifying performance-limiting factors.
- **Advantages:** Offer granular analysis and insights for hardware optimization.

3. Kernel Benchmarks

- **Description:** Focus on key, frequently used code segments within the operating system kernel.
- **Examples:**
    - **Livermore Loops:** A set of scientific computation kernels.
    - **LINPACK:** Linear algebra subroutines, results expressed in MFLOPs/s.
- **Significance:** Help evaluate underlying system performance independent of the specifics of user applications.

4. Synthetic Benchmarks

- **Description:** Designed to simulate workloads by combining instructions based on statistical analysis of real-world programs.
- **Procedure:**
    1. Gather usage statistics from a diverse set of applications.
    2. Determine the proportional mix of instructions/operations.
    3. Construct benchmark code reflecting that mix.
- **Examples**
    - **Whetstone:** Floating-point operations, early standard benchmark.
    - **Dhrystone:** Integer operations, another influential early benchmark.
- **Limitations:** May lose relevance as architectures evolve, need continuous re-calibration.

5. Specialized Benchmarks

- **I/O Benchmarks:** Measure performance of storage subsystems (e.g., disk throughput, latency).
- **Database Benchmarks:** Evaluate DBMS performance with metrics like queries per second, response time under load.
- **Parallel Benchmarks:** Designed for multi-core, multi-processor, or distributed systems; assess scaling and communication overheads.

## Challenges in Benchmarking

- **Vendor Manipulation ("Benchmarketing"):**
    - Systems optimized specifically for known benchmarks, skewing results away from real-world performance representation.
    - Potentially deceptive tactics employed to artificially inflate benchmark scores.
- **Narrow Focus on Computational Speed:**
    - Ignores crucial metrics for broader system evaluation, including :
        - **Qualities of Service:** Security, availability, reliability, scalability.
        - **Total Cost of Ownership (TCO):** Beyond raw performance, consider hardware price, power consumption, and space requirements.
        - **Facilities Burden:** Cooling, power, and space considerations, especially for data center contexts.
- **Benchmarking Complex Systems and Workloads:**
    - Limited benchmarks for distributed systems and grid computing, which are sensitive to network topologies.
    - Difficulty in accurately representing diverse enterprise workloads with varying priorities (e.g., batch jobs, real-time responsiveness).
- **Focus on Averages vs. User Experience:**
    - Benchmarks prioritize mean performance scores but can lack focus on:
        - **Predictability & Service Level Agreements (SLAs):** Vital for real-world scenarios.
        - **Worst-Case Response Times:** Key for real-time systems and user experience.
        - **Performance Degradation at High Usage:** How systems handle near-capacity loads (important to understand "cliff" behaviors).
- **Benchmarking Evolving Landscapes:**
    - Challenges in benchmarking virtualized environments where workloads run concurrently on consolidated servers.
    - Lack of quality benchmarks for high-volume mixed batch and online computing, still crucial for many businesses.
## Benchmarking Principles

- **Relevance:**
    
    - Focus on metrics that directly influence user experience or critical bottlenecks within the system under evaluation.
    - Prioritize measurements that align with the intended use cases and workload characteristics of the systems being tested.
- **Representativeness:**
    
    - Employ industry-standard or widely accepted benchmarks for meaningful comparison.
    - Use benchmark suites that mirror real-world workloads or use cases (ideally, a mix of synthetic and application-based).
- **Equity:**
    
    - Minimize bias in the benchmark selection and execution process.
    - Apply configuration and testing methodologies uniformly across comparable systems.
    - Document any factors that might give a specific system or architecture an unfair advantage.
- **Repeatability:**
    
    - Implement robust experimental design and testing procedures to ensure reproducible results.
    - Control environmental factors (e.g., background processes, system load) to minimize performance variability.
- **Cost-effectiveness:**
    
    - Balance the comprehensiveness of the benchmark suite with the time, effort, and resources required to conduct it.
    - Consider using open-source or readily accessible benchmarks where possible.
- **Scalability:**
    
    - Design benchmarks to assess performance across a range of system configurations (processors, memory, storage).
    - Measure how performance changes with workload increases, particularly for distributed or parallel systems.
- **Transparency:**
    
    - Provide clear documentation of the benchmarks used, the testing methodology, and any system-specific configurations.
    - Disclose any potential conflicts of interest or vendor relationships.

## Common Benchmarking Organizations 

- **Business Applications Performance Corporation (BAPCo)**
    
    - Focus: Performance of business applications on desktop, mobile, and server systems.
    - Key Benchmarks:
        - SYSmark: General productivity and responsiveness suite.
        - MobileMark: Measures battery life and performance on mobile devices.
        - WEBmark: Evaluates web browsing and web application performance.
- **Embedded Microprocessor Benchmark Consortium (EEMBC)**
    
    - Focus: Benchmarks for embedded processors (automotive, networking, consumer electronics).
    - Benchmark Suites:
        - CoreMark: Basic integer processing
        - AutoBench: Automotive industry workloads
        - NetworkingBench: Packet processing performance
        - And more specialized suites for graphics, secure processing, etc.
- **Linked Data Benchmark Council (LDBC)**
    
    - Focus: Performance evaluation of RDF (Resource Description Framework) engines and graph databases.
    - Key Benchmarks:
        - Semantic Publishing Benchmark (SPB): Simulates media and publishing use cases.
        - Social Network Benchmark (SNB): Models social network workloads with interactive queries and analytical tasks.
- **Standard Performance Evaluation Corporation (SPEC)**
    
    - Long-standing organization with broad benchmark coverage.
    - Key Benchmarks:
        - SPECint: Integer-focused performance (diverse workloads)
        - SPECfp: Floating-point-focused (scientific and computational)
        - SPEC CPU: Comprehensive suite targeting modern CPUs
        - SPECpower: Measures server power efficiency under load
        - And many domain-specific suites (graphics, virtualization, Java, HPC)
- **Transaction Processing Performance Council (TPC)**
    
    - Focus: Database Management Systems (DBMS), especially those for OLTP (Online Transaction Processing).
    - Key Benchmarks:
        - TPC-C: Simulates complex OLTP environment
        - TPC-H: Emphasizes decision support and analytical queries
        - TPC-E: Models online brokerage workloads

## Open-Source Benchmarks 

- **General-Purpose Benchmarks**
    - **AIM Multiuser Benchmark:** Mix-and-match test suite to simulate diverse workloads on UNIX-like systems.
    - **Dhrystone, Whetstone:** Classic tests for integer and floating-point performance respectively (DMIPS, MWIPS metrics).
    - **Coremark:** Popular for embedded processors.
    - **NBench:** Synthetic benchmark for CPU integer, memory, and floating-point operations.
- **Storage-Focused Benchmarks**
    - **Bonnie++:** Filesystem and hard drive evaluation.
    - **Data Storage Benchmark:** RDF benchmark targeting graph database technologies.
    - **DiskSpd:** Command-line tool for flexible storage benchmarking.
    - **IOmeter, IOzone:** Target I/O subsystems for single and clustered systems.
- **Specialized Benchmarks**
    - **BRL-CAD:** Ray-tracing suite for cross-platform CPU, compiler, and architecture analysis (used for decades).
    - **DEISA Benchmark Suite:** Scientific HPC applications.
    - **Embench:** Designed for deeply embedded systems with minimal OS support.
    - **Faceted Browsing Benchmark:** Evaluates linked data browsing systems.
    - **Fhourstones:** Integer benchmark.
    - **HINT:** Overall CPU and memory measurement.
    - **Kubestone:** Kubernetes and OpenShift operator benchmarking.
    - **LINPACK benchmarks:** Measure floating-point operations (FLOPS).
    - **Livermore Loops:** Scientific computation kernels.
    - **NAS Parallel Benchmarks:** Suite for evaluating parallel computing systems.
    - **PAL:** Real-time physics engine benchmarking.
    - **Tak (function):** Simple test for recursion performance.
    - **TATP Benchmark:** Telecommunications-focused workload.
    - **TPoX:** XML database transaction processing.
    - **VUP (VAX unit of performance):** Older comparison metric.
- **Frameworks**
    - **Collective Knowledge:** Crowdsources and optimizes workloads across diverse volunteer-provided hardware (great for deep learning).
    - **PerfKitBenchmarker:** Measures and compares cloud offerings.
    - **Phoronix Test Suite:** Flexible cross-platform suite incorporating many other benchmarks.
## Windows Benchmarking Tools 

**General System Performance**

- **BAPCo:** Productivity, responsiveness, and web-based workload suites.
    - **MobileMark:** Focuses on battery life and performance for mobile devices.
    - **SYSmark:** Broader mix of real-world applications.
    - **WebMark:** Browsing and web application scenarios.
- **Futuremark**
    - **PCMark:** General system benchmark with tasks ranging from office work to gaming.
    - **3DMark:** Specialized for graphics performance (gaming focus).
- **PassMark Software:** Offers a comprehensive suite with CPU, memory, storage, and graphics tests.
- **Windows System Assessment Tool (WinSAT):** Built into Windows Vista and later; provides a basic index for users.

**Storage Focused**

- **CrystalDiskMark:** Measures sequential and random read/write speeds for various storage devices.

**CPU-Specific Benchmarks**

- **PiFast, SuperPrime, Super PI** : Calculate Pi to a large number of digits, stressing the CPU's floating-point capabilities.
- **Whetstone:** Older benchmark focused on floating-point operations.

**Other**

- **UserBenchmark:** Crowdsourced benchmark platform; good for rough comparisons, but use caution with absolute scores due to variability.
- **Worldbench (Discontinued):** Previously popular suite for overall system evaluation.
## Other Notable Benchmarks 

- **Mobile Focused**
    
    - **AnTuTu:** Popular for Android and ARM-based devices, offering a comprehensive score covering CPU, GPU, memory, and user experience.
- **SPARQL / Semantic Web**
    
    - **Berlin SPARQL Benchmark (BSBM):** Evaluate the performance of SPARQL storage solutions across different architectures.
    - **Lehigh University Benchmark (LUBM):** Tests Semantic Web repositories using complex queries over large, realistic datasets.
- **General Purpose**
    
    - **Geekbench:** Cross-platform benchmark (Windows, macOS, Linux, mobile) covering integer, floating-point, and memory-focused tests.
- **Historical / Vendor-Specific**
    
    - **iCOMP:** Intel's Comparative Microprocessor Performance index, now largely replaced by more generic benchmarks.
    - **Performance Rating:** AMD and Cyrix's modeling scheme for relative performance comparisons (primarily of historical interest).
- **Specialized Workloads**
    
    - **Khornerstone:** An older benchmark emphasizing database-style operations.
    - **SunSpider:** Browser-focused JavaScript benchmark.
    - **VMmark:** Virtualization performance evaluation suite.
    - **RenderStats:** Database of 3D rendering benchmark results, valuable for graphics analysis.