# Amdahl's Law

In computer architecture, **Amdahl's law** (or **Amdahl's argument**) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved.

$$ S = \frac{1}{(1 - P) + \frac{P}{N}} $$
## Key Components of the Equation

- $S$: The factor by which your overall system speeds up after improvement (_Speedup = Time_old / Time_new_). A speedup of 2 implies the system becomes twice as fast.
- $P$: The fraction of the program or task that can be improved or parallelized. This ranges from 0 (no part is improvable) to 1 (the entire task can be enhanced).
- $N$: The speedup factor of the enhanced portion, or the number of processors used if you're parallelizing a task.

Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors.

> [!example]
>  If a program needs 20 hours to complete using a single thread, but a one hour portion of the program cannot be parallelized, therefore only the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many threads are devoted to a parallelized execution of this program, the minimum execution time cannot be less than one hour. Hence, the theoretical speedup is limited to at most 20 times the single thread performance.$(\frac{1}{1-p}=20)$

![[Pasted image 20240310162459.png]]
## Intuition Behind the Law 

Imagine a task takes 100 seconds to complete. If 80 seconds of that task can potentially be made 4 times faster through parallelization, and 20 seconds are inherently serial:

- **Best-Case Scenario:** The parallel part now takes 80 / 4 = 20 seconds.
- **Overall Time:** 20 (parallel) + 20 (serial) = 40 seconds
- **Speedup:** 100 (original) / 40 (new) = 2.5x

Even though you improved a significant chunk by 4x, the fixed serial portion limits your overall speedup.